{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines...\n",
      "There are 15000000 lines in this text file. Is each line a tweet?\n",
      "1500000 / 15000000\n",
      "3000000 / 15000000\n",
      "4500000 / 15000000\n",
      "6000000 / 15000000\n",
      "7500000 / 15000000\n",
      "9000000 / 15000000\n",
      "10500000 / 15000000\n",
      "12000000 / 15000000\n",
      "13500000 / 15000000\n",
      "15000000 / 15000000\n",
      "Hashing.\n",
      "Building trees.\n",
      "Building paths.\n",
      "There were 39487 trees built.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, hashlib, pickle\n",
    "file = 'O:/sam_data/sam.tsv'\n",
    "file = open(file, \"r\", encoding = 'utf-8')\n",
    "print('Counting lines...')\n",
    "number_of_tweets = 15000000#100000\n",
    "if number_of_tweets==0:\n",
    "    number_of_tweets = sum(1 for line in file)  #this is the total number of available tweets\n",
    "print('There are '+str(number_of_tweets)+' lines in this text file. Is each line a tweet?')\n",
    "interval_size = int(number_of_tweets/10) # this is just to show progress\n",
    "\n",
    "\n",
    "# there are quite some garbage lists here\n",
    "counter     = 0\n",
    "pairs       = []\n",
    "altPairs    = []\n",
    "pairsWithSentiment = []\n",
    "lenAltPairs = []\n",
    "exceptions  = []\n",
    "sentiments = []\n",
    "hashes = []\n",
    "\n",
    "for line in file:\n",
    "    #print(line)\n",
    "    counter=counter+1\n",
    "    if counter%interval_size==0:\n",
    "        print(str(counter)+' / '+str(number_of_tweets))\n",
    "    #m = re.search(r'\\t([^\\t]+)\\thttp[^\\t]+\\t([^\\t]+)\\t',line)\n",
    "    m = re.search(r'([0-9]\\.[0-9]+)\\t[^\\t]+\\t[^\\t]+\\t[^\\t]+\\t([^\\t]+)\\thttp[^\\t]+\\t([^\\t]+)\\t',line)\n",
    "    if counter==1:\n",
    "        typeHolder = type(m)\n",
    "    #print(type(m))\n",
    "    if type(m)==typeHolder:\n",
    "        username = m.group(2)\n",
    "        msgsearch = re.search('T @([^:]+): ([^\\t]+)',m.group(3))\n",
    "        \n",
    "        #print(retweetee.group(1))\n",
    "        try:\n",
    "            sentiment = m.group(1)\n",
    "            retweetee = msgsearch.group(1)\n",
    "            #print(str(username)+' '+str(retweetee))\n",
    "        except:\n",
    "            if type(retweetee)!=str:\n",
    "                retweetee='NONE'\n",
    "        try:\n",
    "            msg = msgsearch.group(2)\n",
    "        except:\n",
    "            msg = m.group(3)\n",
    "        #, m.group(3))\n",
    "        #print(pair)\n",
    "        msgHash = hashlib.sha1(msg.encode(\"utf-8\")).hexdigest()\n",
    "        \n",
    "        #print(msg[0:30],msgHash)\n",
    "        sender = retweetee\n",
    "        receiver = username\n",
    "        \n",
    "        pair = [msgHash, str(sender), str(receiver)]\n",
    "        hashes.append(msgHash)\n",
    "        pairs.append(pair)\n",
    "        sentiments.append(sentiment)\n",
    "        exceptions.append(m)\n",
    "    else:\n",
    "        exceptions.append(m)\n",
    "    #print([username, retweetee, sentiment])\n",
    "    if counter==number_of_tweets:\n",
    "        break\n",
    "    \n",
    "print('Hashing.')\n",
    "order = sorted(range(len(hashes)), key=lambda k: hashes[k])\n",
    "sortedHashes = []\n",
    "sortedPairs = []\n",
    "sortedSentiments = []\n",
    "for i in order:\n",
    "    sortedHashes.append(hashes[i])\n",
    "    sortedPairs.append(pairs[i])\n",
    "    sortedSentiments.append(sentiments[i])\n",
    "    \n",
    "#for pair in sortedPairs:\n",
    "    #print(pair[0][::5],pair[1],pair[2])\n",
    "print('Building trees.')    \n",
    "trees = []\n",
    "s = []\n",
    "bucket = []\n",
    "counter = 0\n",
    "for i in range(0,len(sortedPairs)-1):\n",
    "\n",
    "    leadingPair = sortedPairs[i]\n",
    "    followingPair = sortedPairs[i+1]\n",
    "    #print(leadingPair,followingPair)\n",
    "    if leadingPair[0]==followingPair[0]: #if following hashes match\n",
    "        counter += 1\n",
    "        bucket.append([leadingPair[1],leadingPair[2]])\n",
    "        #print(bucket)\n",
    "    else:\n",
    "        trees.append(bucket)\n",
    "        s.append(sortedSentiments[i])\n",
    "        bucket = []\n",
    "        \n",
    "### get max tree\n",
    "'''\n",
    "max = 0\n",
    "for tree in trees:\n",
    "    if len(tree)>max:\n",
    "        max = len(tree)\n",
    "        maxTree = tree\n",
    "print(maxTree)\n",
    "print(max, counter)\n",
    "'''\n",
    "### add edges\n",
    "import networkx as nx\n",
    "\n",
    "paths = []\n",
    "ps = []\n",
    "print('Building paths.')\n",
    "tree_counter = 0\n",
    "for i in range(0,len(trees)):\n",
    "    tree = trees[i]\n",
    "    if len(tree)>10:\n",
    "        tree_counter += 1\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(tree)\n",
    "        creator = []\n",
    "        endpoints = []\n",
    "        maxDeg = 0\n",
    "        for node in G.nodes():\n",
    "            deg = G.degree(node)\n",
    "            if deg==1:\n",
    "                endpoints.append(node)\n",
    "            if deg>maxDeg:\n",
    "                creator = node\n",
    "        for endpoint in endpoints:\n",
    "            try:\n",
    "                p = nx.dijkstra_path(G, creator, endpoint)\n",
    "                if len(p)>1:\n",
    "                    paths.append(p)\n",
    "                    ps.append(s[i])\n",
    "            except:\n",
    "                continue\n",
    "                #print('no path')\n",
    "\n",
    "print(\"There were \"+str(tree_counter)+\" trees built.\")\n",
    "      \n",
    "with open('paths_with_sentiment.pkl', 'wb') as f:\n",
    "    pickle.dump([paths,ps], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding and vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data built! W2V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OP\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing Word2Vec model 'n_30features_3minwords_2context'\n",
      "Vocabulary Size: 450170\n",
      "Initializing embedding layer with word2vec weights, shape (450170, 30)\n",
      "One rule: 0.4994596434309659\n",
      "Train on 226239 samples, validate on 226240 samples\n",
      "Epoch 1/1\n",
      "226239/226239 [==============================] - 1872s - loss: 0.2415 - acc: 0.8887 - val_loss: 0.0348 - val_acc: 0.9896\n",
      "Model saved!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, hashlib, pickle\n",
    "\n",
    "pickle_in = open('paths_with_sentiment_balanced.pkl',\"rb\")\n",
    "[paths, ps] = pickle.load(pickle_in)\n",
    "\n",
    "split = 0.5\n",
    "\n",
    "### PADDING\n",
    "print('Padding and vocabulary')\n",
    "maxLen = 0\n",
    "for i in range(0,len(paths)):\n",
    "    if len(paths[i])>maxLen:\n",
    "        maxLen = len(paths[i])\n",
    "for i in range(0,len(paths)):\n",
    "    if len(paths[i])<maxLen:\n",
    "        diff = maxLen-len(paths[i])\n",
    "        for j in range(0,diff):\n",
    "            paths[i].append(\"<PAD/>\")\n",
    "### VOCABULARY        \n",
    "def build_vocab(sentences):\n",
    "    from collections import Counter\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build \n",
    "    print(sentences)\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "[vocabulary, vocabulary_inv_list] = build_vocab(paths)\n",
    "### Dummy labels\n",
    "'''\n",
    "labels = []\n",
    "for path in paths:\n",
    "    labels.append(random.randint(0,1))\n",
    "'''\n",
    "labels = []\n",
    "for score in ps:\n",
    "    fscore = float(score)\n",
    "    if fscore>0.5:\n",
    "        labels.append(1)\n",
    "    if fscore<=0.5:\n",
    "        labels.append(0)\n",
    "### build np data\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "[x, y] = build_input_data(paths, labels, vocabulary)\n",
    "### transform to training data\n",
    "vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "train_len = int(len(x) * split)\n",
    "x_train = x[:train_len]\n",
    "y_train = y[:train_len]\n",
    "x_test = x[train_len:]\n",
    "y_test = y[train_len:]\n",
    "print('Input data built! W2V')\n",
    "### TRAIN WORD TO VEC!\n",
    "from w2v import train_word2vec\n",
    "min_word_count = 3\n",
    "context = 2\n",
    "embedding_dim = 30\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), 'node_embeddings', vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "\n",
    "### NN\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, ZeroPadding1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import callbacks\n",
    "from sklearn import metrics\n",
    "# Model Hyperparameters\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 20 #10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 30\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "sequence_length = 3 #400\n",
    "max_words = 5000\n",
    "### Part 2B: Network definition & word2vec training\n",
    "### make sure to delete existing word2vec model if you want to udate it\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    sequence_length = x_test.shape[1]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = ZeroPadding1D(padding = int((sz-1)/2))(z)\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                            kernel_size=sz,\n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\",\n",
    "                            strides=1)(conv)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "embedding_layer.set_weights([weights])\n",
    "    \n",
    "pos=0\n",
    "tot=len(labels)\n",
    "for label in labels:\n",
    "    if label==1:\n",
    "        pos += 1\n",
    "print('One rule: '+str(pos/tot))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=5, epochs=num_epochs, validation_data = (x_test, y_test), verbose=1)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('./models/'+'nodes'+'_json'+'.json','w') as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "\t##Serialize model weights to HDF5\n",
    "model.save_weights('./models/'+'nodes'+'_weights'+'.h5')\n",
    "print('Model saved!!')\n",
    "\n",
    "val_accu = model.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Accuracy = \" + str(val_accu[1]))\n",
    "\n",
    "y_confuse = model.predict(x_test,batch_size = batch_size,verbose = 1)\n",
    "for i in range(len(y_confuse)):\n",
    "    if y_confuse[i]>0.5:\n",
    "        y_confuse[i] = 1\n",
    "    else:\n",
    "        y_confuse[i] = 0\n",
    "\n",
    "confuse_matrix = metrics.confusion_matrix(y_test, y_confuse)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(str(confuse_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
