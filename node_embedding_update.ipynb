{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data built! W2V\n",
      "Load existing Word2Vec model 'n_30features_3minwords_2context'\n",
      "Vocabulary Size: 450587\n",
      "Initializing embedding layer with word2vec weights, shape (450587, 30)\n",
      "Train on 226005 samples, validate on 226827 samples\n",
      "Epoch 1/1\n",
      "226005/226005 [==============================] - 1856s - loss: 0.2610 - acc: 0.8723 - val_loss: 0.0426 - val_acc: 0.9878\n",
      "Model saved!!\n",
      "Accuracy = 0.987818910447\n",
      "226048/226827 [============================>.] - ETA: 0s\n",
      "Confusion Matrix:\n",
      "[[111941   1806]\n",
      " [   957 112123]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#cd C:\\Users\\OP\\OneDrive\\APG\\code\\ingress\n",
    "pickle_in = open('training_data_all.pkl',\"rb\")\n",
    "[x_train, x_test, y_train ,y_test, vocabulary_inv] = pickle.load(pickle_in)\n",
    "print('Input data built! W2V')\n",
    "### TRAIN WORD TO VEC!\n",
    "from w2v import train_word2vec\n",
    "min_word_count = 3\n",
    "context = 2\n",
    "embedding_dim = 30\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), 'node_embeddings', vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "\n",
    "### NN\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, ZeroPadding1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import callbacks\n",
    "from sklearn import metrics\n",
    "# Model Hyperparameters\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 20 #10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 30\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "sequence_length = 3 #400\n",
    "max_words = 5000\n",
    "### Part 2B: Network definition & word2vec training\n",
    "### make sure to delete existing word2vec model if you want to udate it\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    sequence_length = x_test.shape[1]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = ZeroPadding1D(padding = int((sz-1)/2))(z)\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                            kernel_size=sz,\n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\",\n",
    "                            strides=1)(conv)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding\")\n",
    "embedding_layer.set_weights([weights])\n",
    "    \n",
    "model.fit(x_train, y_train, batch_size=5, epochs=num_epochs, validation_data = (x_test, y_test), verbose=1)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('./models/'+'nodes'+'_json'+'.json','w') as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "\t##Serialize model weights to HDF5\n",
    "model.save_weights('./models/'+'nodes'+'_weights'+'.h5')\n",
    "print('Model saved!!')\n",
    "\n",
    "val_accu = model.evaluate(x_test,y_test,verbose = 0)\n",
    "print(\"Accuracy = \" + str(val_accu[1]))\n",
    "\n",
    "y_confuse = model.predict(x_test,batch_size = batch_size,verbose = 1)\n",
    "for i in range(len(y_confuse)):\n",
    "    if y_confuse[i]>0.5:\n",
    "        y_confuse[i] = 1\n",
    "    else:\n",
    "        y_confuse[i] = 0\n",
    "\n",
    "confuse_matrix = metrics.confusion_matrix(y_test, y_confuse)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(str(confuse_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
